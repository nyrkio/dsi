# Defaults for all config values used by Distributed Performance 2.0 modules.

bootstrap:
  infrastructure_provisioning: single
  platform: linux
  mongodb_setup: standalone
  workload_setup: common
  analysis: common
  storageEngine: wiredTiger
  test_control: ycsb
  production: false

infrastructure_provisioning:
  # Write hostnames to /etc/hosts on deployed hosts. (Uses private ip:s when available.)
  hostnames:
    # Supported methods: /etc/hosts and null
    method: /etc/hosts
    domain: dsitest.dev
  # Terraform variables
  tfvars:
    cluster_name: default_cluster_name
    mongod_instance_count: 0
    mongod_ebs_instance_count: 1
    workload_instance_count: 1

    mongod_ebs_instance_type: c7i.8xlarge
    mongod_instance_type: c7i.8xlarge
    workload_instance_type: c7i.8xlarge

    image: ami-0a70b9d193ae8a799
    linux_distro: amazon2
    region: us-west-2
    availability_zone: us-west-2a

    ssh_user: ec2-user
    ssh_key_name: serverteam-perf-ssh-key
    ssh_key_file: aws_ssh_key.pem

    tags:
      expire-on-delta: 2     # adjust expire_on to now + expire-on-delta hours
      owner: perf-terraform-alerts@10gen.com
  # numactl_prefix: numactl --interleave=all --cpunodebind=1
  # c7i.8xlarge only has 1 socket, this is no longer needed
  numactl_prefix: ""
  terraform:
    required_version: Terraform v1.7.5
    aws_required_version: 5.43.0
    linux_download: https://releases.hashicorp.com/terraform/1.7.5/terraform_1.7.5_linux_amd64.zip
    mac_download: https://releases.hashicorp.com/terraform/1.7.5/terraform_1.7.5_darwin_amd64.zip
    # required_version: Terraform v0.12.16
    # aws_required_version: 2.40.0
    # linux_download: https://releases.hashicorp.com/terraform/0.12.16/terraform_0.12.16_linux_amd64.zip
    # mac_download: https://releases.hashicorp.com/terraform/0.12.16/terraform_0.12.16_darwin_amd64.zip

  # Meta-data about this instance type. Can be used for example to parameterize later configuration
  # so that it is based on number of cpu's or memory.
  meta:
    nodes_in_cluster: 3 # Counting mongod instance, not wc
    cpu_per_host: 16 # Note: we turn off hyperthreading
    cpu_total_cluster: 48
    memory_per_cpu_gb: 4
    memory_per_host_gb: 64
    memory_total_cluster: 196



  # System related defaults for network_delays command
  network_delays:
    # Currently only a single device is supported here
    interface: eth0
    sys_configs:
      amazon2:
        # In the normal case, we use an absurdly high rate limit that is never hit.
        rate: 100tbit
      centos7:
        # This kernel/tc version only allows this max rate.
        rate: 4.2gbps

  # Known disks in the system. This list is a superset. String list for use below
  disks: "/dev/xvdc /dev/xvdd /dev/xvde /dev/xvdf /dev/nvme0n1 /dev/nvme1n1 /dev/nvme2n1 /dev/nvme0n1"
  read_ahead: 32
  post_provisioning_steps:
    # This is a dict of steps you might want to use. This is not used directly.
    set_readahead:
      exec: |
        # Set readahead: https://docs.mongodb.com/manual/administration/production-notes/#recommended-configuration
        # (noatime is set when mounting.)
        for disk in ${infrastructure_provisioning.disks}; do
          if [[ -e $disk ]]; then
            sudo blockdev --setra ${infrastructure_provisioning.read_ahead} $disk;
          fi;
        done;
    set_ulimits:
      exec: |
        # set ulimit nofile for user
        echo "${infrastructure_provisioning.tfvars.ssh_user}           soft    nofile          65536" | sudo tee -a /etc/security/limits.conf > /dev/null
        echo "${infrastructure_provisioning.tfvars.ssh_user}           hard    nofile          65536" | sudo tee -a /etc/security/limits.conf > /dev/null
        echo "${infrastructure_provisioning.tfvars.ssh_user}           soft    nproc           65536" | sudo tee -a /etc/security/limits.conf > /dev/null
        echo "${infrastructure_provisioning.tfvars.ssh_user}           hard    nproc          65536" | sudo tee -a /etc/security/limits.conf > /dev/null
        echo "${infrastructure_provisioning.tfvars.ssh_user}   soft   core   unlimited" | sudo tee -a /etc/security/limits.conf > /dev/null
        echo "${infrastructure_provisioning.tfvars.ssh_user}   hard   core   unlimited" | sudo tee -a /etc/security/limits.conf > /dev/null
        # For control of core dumps and file names:
        # http://man7.org/linux/man-pages/man5/core.5.html
        mkdir -p "$HOME/data/logs"
        echo "$HOME/data/logs/core.%e.%p.%h.%t" |sudo tee -a  /proc/sys/kernel/core_pattern > /dev/null
        echo vm.max_map_count = 262144 | sudo tee -a /etc/sysctl.d/99-dsi.conf > /dev/null
        sudo sysctl -w vm.max_map_count=262144
    production_settings:
      exec: |
        # mongodb production recommended configuration
        # https://docs.mongodb.com/manual/administration/production-notes/#recommended-configuration
        echo 'never' | sudo tee /sys/kernel/mm/transparent_hugepage/enabled > /dev/null
        echo 'never' | sudo tee /sys/kernel/mm/transparent_hugepage/defrag > /dev/null

    install_stuff:
      exec: |
        sudo yum update --quiet
        sudo yum install -y --quiet iproute-tc numactl nano git > /dev/null

  post_provisioning:
    - on_all_hosts: ${infrastructure_provisioning.post_provisioning_steps.install_stuff}
    - on_all_hosts: ${infrastructure_provisioning.post_provisioning_steps.set_readahead}
    - on_all_hosts: ${infrastructure_provisioning.post_provisioning_steps.set_ulimits}
    - on_all_hosts: ${infrastructure_provisioning.post_provisioning_steps.production_settings}


workload_setup:
  # Paths to other repos on your workstation
  local_repos:
    workloads: ./workloads
    ycsb: ./YCSB
    linkbench: ./linkbench
    tpcc: ./tpcc
    genny: ./data/genny  # Put genny on the large drive to give it more space.
    benchmarks: ./benchmarks  # sysbench


mongodb_setup:
  mongo_dir: mongodb
  journal_dir: /media/ephemeral1/journal
  clean_db_dir: true
  # Note: It's also allowed to upload your own binary. You can unset this by setting to the empty
  # string "" in mongodb_setup.yml or overrides.yml.
  # This URL is 4.2.2. We use the sys-perf build instead of official release to include the jstests/ directory that we copy in our compile task.
  # mongodb_binary_archive: https://s3.amazonaws.com/mciuploads/dsi/sys_perf_4.2_a0bbbff6ada159e19298d37946ac8dc4b497eadf/a0bbbff6ada159e19298d37946ac8dc4b497eadf/linux/mongodb-enterprise-sys_perf_4.2_a0bbbff6ada159e19298d37946ac8dc4b497eadf.tar.gz
  mongodb_binary_archive: https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-amazon2023-7.0.7.tgz

  mongod_config_file:  # Note these defaults can be overridden by user, but not unset.
    net:
      port: 27017
      bindIp: 0.0.0.0
    processManagement:
      fork: true
    setParameter:
      enableTestCommands: 1
    storage:
      dbPath: data/dbs
      engine: wiredTiger
    systemLog:
      destination: file
      path: data/logs/mongod.log

  authentication:
    enabled: true
    username: username
    password: password
    # Part to use in mongodb_url
    url:
      "True": "${mongodb_setup.authentication.username}:${mongodb_setup.authentication.password}@"
      "False": ""

  # Upload SSL keys (not always used but no harm in uploading them)
  pre_cluster_start:
    - on_all_hosts:
        exec: |
          rm -rf mongosh-*
    - on_all_hosts:
        download_files:
          - source: https://downloads.mongodb.com/compass/mongosh-2.2.2-linux-x64-openssl3.tgz
            target: mongosh-dsimanagedversion.tgz
    - on_all_hosts:
        exec: |
          tar xf mongosh-dsimanagedversion.tgz
          mkdir -p bin
          # Backward compatibility
          ln -s $(pwd)/mongosh-*/bin/mongosh bin/mongo
          bin/mongo --version


    - on_all_hosts:
        upload_repo_files:
          - source: configurations/mongodb_setup/ssl/dsitest.dev.pem
            target: ${mongodb_setup.mongo_dir}/member.pem
          - source: configurations/mongodb_setup/ssl/dsitest.dev.ca-bundle
            target: ${mongodb_setup.mongo_dir}/root.crt

  mongos_config_file:
    net: ${mongodb_setup.mongod_config_file.net}
    processManagement:
      fork: true
    setParameter:
      enableTestCommands: 1
    systemLog:
      destination: file
      path: data/logs/mongos.log

  rs_conf:
    settings:
      chainingAllowed: false

  configsvr_config_file:
    net: ${mongodb_setup.mongod_config_file.net}
    processManagement:
      fork: true
    setParameter:
      enableTestCommands: 1
    storage:
      dbPath: data/dbs
      engine: wiredTiger
    systemLog:
      destination: file
      path: data/logs/mongod.log

  # these options are passed to the mongo shutdownServer command
  shutdown_options:
    force: true
    timeoutSecs: 30

  # these timeouts specify the max amount of milliseconds that the mongo shutdownServer command and
  # pkill mongo commands can run for.
  timeouts:
    shutdown_ms: 540000
    sigterm_ms: 60000

  meta:
    # A single host, as in "host:port". Use the primary or first mongos
    hostname: md
    # The list of hosts that can be used in a mongodb connection string
    hosts: md:27017
    # This lets you easily append &replicaSet=foo or other url params
    # but still keep the base url with authentication etc.
    # This param in general shouldn't be overwritten.
    # This should *not* have any query-string parts in it - downstream is expecting to be the first to add on a ?
    mongodb_url_base: "mongodb://${mongodb_setup.authentication.url.${mongodb_setup.authentication.enabled}}${mongodb_setup.meta.hosts}/admin"
    # Most clients will use this to connect to the cluster
    mongodb_url: "${mongodb_setup.meta.mongodb_url_base}?ssl=false"
    # We need to provide a separate variable for the shell, because the shell on 3.4 doesn't
    # support connecting to multiple mongoses and therefore can't use mongodb_url.
    mongodb_shell_url: "mongodb://${mongodb_setup.authentication.url.${mongodb_setup.authentication.enabled}}${mongodb_setup.meta.hostname}:27017/admin?ssl=false"
    # Used in mongoshell workloads (https://github.com/10gen/workloads/blob/23b1c6dd3a8c087b6a2c949147a8aeaba1d1d271/run_workloads.py#L289-L296)
    shell_ssl_options: []
    connection_string: ${mongodb_setup.meta.mongodb_shell_url}
    is_sharded: false
    is_replset: false
    is_fle: false
    secondary: null
    is_atlas: false
    # Clients (e.g. test_control) should use this to pick up net.ssl settings. Since there are
    # several of them, we just pull the entire net.ssl here.
    net: ${mongodb_setup.mongod_config_file.net}
    # Meta-data used to determine scale_factor for a cluster
    storageEngine: ${mongodb_setup.mongod_config_file.storage.engine}
    # Number of primaries in the cluster. For non-sharded this
    # should be 1. For sharded will be the number of shards.
    primaries: 1
    secondaries: 0  # Secondaries per shard
    jdbc:
      connection_string: jdbc:mysql://${mongodb_setup.meta.hostname}:27017

  post_test:
    - on_all_servers:
        retrieve_files:
          - source: data/logs/
            target: ./
    - on_mongod:
        retrieve_files:
          - source: data/dbs/diagnostic.data
            target: ./diagnostic.data
    - on_configsvr:
        retrieve_files:
          - source: data/dbs/diagnostic.data
            target: ./diagnostic.data

  post_task:
    - on_all_servers:
        exec: |
          rm -rf ./mdiag
          mkdir -p ./mdiag
          # TODO: After the PR for --fast option is merged, this can be changed to upstream with s/henrikingo/mongodb/
          wget -q -O-  https://raw.githubusercontent.com/henrikingo/support-tools/master/mdiag/mdiag.sh > ./mdiag/mdiag.sh
          # The output is written to "$TMPDIR/mdiag-$(hostname).json".
          # This json file is machine readable, rendering the
          # output to a more hunan friendly format is not done here.
          echo "Collecting config and perf metrics with mdiag."
          sudo TMPDIR=./mdiag bash ./mdiag/mdiag.sh --inhibit-new-version-check --fast > /dev/null
    - on_all_servers:
        retrieve_files:
          - source: mdiag/
            target: ./

  upon_error:
    - on_all_servers:
        exec: |
          (df; du  --max-depth 1 ./data) | sed "s/^/$(hostname): /"
        retrieve_files:
          - source: data/logs/
            target: ./
    - on_mongod:
        retrieve_files:
          - source: data/dbs/diagnostic.data
            target: ./diagnostic.data
    - on_configsvr:
        retrieve_files:
          - source: data/dbs/diagnostic.data
            target: ./diagnostic.data

# Cluster setup is a generalization of mongodb_setup. DSI can support any distributed system that:
# - May or may not be a database, and therefore have a journal and data directory
# - Consists of nodes, replica sets and shards
# Can be downloaded and deployed from a tarball
#
# The first non-mongodb product to be supported by DSI was CrateDB. (Work sponsored by them!)
cluster_setup:
  # Binaries that we want to test
  tar_file_url: ${cluster_setup.product.${cluster_setup.meta.product_name}.tar_file_url}
  # Wheter to use numactl_prefix or not
  numactl: ${cluster_setup.product.${cluster_setup.meta.product_name}.numactl}

  # One or more program names started by cluster_setup.py
  # This list is mostly used for pgrep and pkill. The actual start command is in the next row
  launch_program: ${cluster_setup.product.${cluster_setup.meta.product_name}.launch_program}
  launch_command: ${cluster_setup.product.${cluster_setup.meta.product_name}.launch_command}
  shutdown_command: ${cluster_setup.product.${cluster_setup.meta.product_name}.shutdown_command}
  client_command: ${cluster_setup.product.${cluster_setup.meta.product_name}.client_command}
  client_command_file: ${cluster_setup.product.${cluster_setup.meta.product_name}.client_command_file}
  client_command_local: ${cluster_setup.product.${cluster_setup.meta.product_name}.client_command_local}
  client_command_local_file: ${cluster_setup.product.${cluster_setup.meta.product_name}.client_command_local_file}
  check_node_up: ${cluster_setup.product.${cluster_setup.meta.product_name}.check_node_up}

  files:
    lock: ${cluster_setup.directories.data_dir}/${cluster_setup.launch_program.0}.lock
  directories: ${cluster_setup.product.${cluster_setup.meta.product_name}.directories}

  clean_db_dir: true
  clean_logs: true

  node_config_file: ${cluster_setup.product.${cluster_setup.meta.product_name}.node_config_file}

  authentication:
    enabled: false
    username: username
    password: password
    url:
      "True": "${cluster_setup.authentication.username}:${cluster_setup.authentication.password}@"
      "False": ""


  topology:
    - cluster_type: standalone
      id: myid1
      node:
        # TODO: in the following 2 lines mongod is simply a terraform host type. No mongod is actually running there.
        public_ip: ${infrastructure_provisioning.out.mongod.0.public_ip}
        private_ip: ${infrastructure_provisioning.out.mongod.0.private_ip}


  # Meta data about this cluster setup
  meta:
    # Note: This key is rather significant, as a lot of other defaults are product specific.
    # So you might find the active cluster_setup.shutdown_command is actually defined as
    # cluster_setup.cratedb.shutdown_command.
    # For backward compatibility the default value is mongodb, which is kind of funny because
    # in that case cluster_setup.yml isn't even used, rather mongodb_setup.yml.
    product_name: mongodb

    cluster_setup: standalone
    # The list of hosts that can be used in a driver's connection string
    hosts: ${cluster_setup.product.${cluster_setup.meta.product_name}.meta.hosts}
    # A single host, as in "host:port". Use the primary or first proxy / entry point
    hostname: ${cluster_setup.product.${cluster_setup.meta.product_name}.meta.hostname}
    connection_string: ${cluster_setup.product.${cluster_setup.meta.product_name}.meta.connection_string}
    port: ${cluster_setup.product.${cluster_setup.meta.product_name}.meta.port}
    pgsql:
      port: 5432
    jdbc: ${cluster_setup.product.${cluster_setup.meta.product_name}.meta.jdbc}


    is_sharded: false
    is_replset: false
    # Number of primaries in the cluster. For non-sharded this
    # should be 1. For sharded will be the number of shards.
    primaries: 1
    secondaries: 0  # Secondaries per shard



  pre_cluster_start: ${cluster_setup.product.${cluster_setup.meta.product_name}.pre_cluster_start}
  post_cluster_start: ${cluster_setup.product.${cluster_setup.meta.product_name}.post_cluster_start}
  post_test: ${cluster_setup.product.${cluster_setup.meta.product_name}.post_test}


  # Default configs, by product name
  # Note these defaults can be overridden by user, but not unset.
  product:
    mongodb:
      meta: ${mongodb_setup.meta}
      numactl: false
      tar_file_url: not used
      launch_program: not used
      launch_command: not used
      shutdown_command: not used
      client_command: ./bin/mongo --quiet {connection_string}
      client_command_file: ./bin/mongo --quiet {connection_string} {command_file}
      client_command_local: ./bin/mongo --quiet
      client_command_local_file: ./bin/mongo --quiet
      check_node_up: not used
      files:
        lock: not used
      directories:
        extract_dir: not used
        bin_dir: not used
        log_dir: not used
        data_dir: not used
        config_dir: not used
        journal: not used
      node_config_file: ${mongodb_setup.mongod_config_file}
      pre_cluster_start:
        - on_localhost:
            exec: echo "Default message"
      post_cluster_start:
        - on_localhost:
            exec: echo "Default message"
      post_test:
        - on_all_servers:
            retrieve_files:
              - source: ${cluster_setup.directories.log_dir}
                target: ./

    cratedb:
      tar_file_url: https://cdn.crate.io/downloads/releases/cratedb/x64_linux/crate-5.6.2.tar.gz

      numactl: false

      launch_program:
        - crate
        - java

      # Recent CrateDB versions no longer support the --daemonize/--fork option at all. So we need to
      # use systemctl even if everything is "installed in ec2-user home directory"
      launch_command: sudo systemctl daemon-reload; sudo systemctl start crate.service
      shutdown_command: sudo systemctl stop crate.service
      client_command: ./crash --format dynamic --hosts {connection_string} --command "{command}"
      client_command_file: ./crash --format dynamic --hosts {connection_string} --command "\r {command_file}"
      # It seems I can only make crate listen on external address or localhost, but not both
      # Hence using $(hostname) as a crutch for localhost
      client_command_local: ./crash --format dynamic --hosts $(hostname):4200 --command "{}"
      client_command_local_file: ./crash --format dynamic --hosts $(hostname):4200 --command "\r {}"
      check_node_up: SELECT attributes, cluster_state_version, connections,version FROM sys.nodes;
      files:
        lock: ${cluster_setup.directories.data_dir}/${cluster_setup.launch_program.0}.lock
      directories:
        extract_dir: crate
        bin_dir: crate/bin
        log_dir: logs
        data_dir: data
        config_dir: crate/config
        journal: /media/ebs2/journal

      node_config_file:
        remote_path: crate/config/crate.yml
        content: |
          path:
            data: data
            logs: logs
          network:
            host: _site_
          auth:
            host_based:
              config:
                0:
                  user: crate
                  address: _site_
                  method: trust
                99:
                  method: password


      meta:
        hosts: ${cluster_setup.topology.0.node.0.private_ip}:${cluster_setup.meta.port}
        hostname: ${cluster_setup.topology.0.node.0.private_ip}
        connection_string: ${cluster_setup.topology.0.node.0.private_ip}:${cluster_setup.meta.port}
        port: 4200
        is_sharded: false
        is_replset: false
        cluster_setup: standalone
        pgsql:
          port: 5432
        jdbc:
          connection_string: jdbc:postgresql://${cluster_setup.meta.hostname}:${cluster_setup.meta.pgsql.port}

      pre_cluster_start:
        - on_all_hosts:
            upload_files:
              - target: crate.service
                content: |
                  [Unit]
                  Description=CrateDB Server
                  Documentation=https://crate.io/docs/
                  Wants=network.target
                  After=network.target

                  [Service]
                  Type=simple
                  User=ec2-user
                  Group=ec2-user
                  WorkingDirectory=/home/ec2-user/crate
                  Environment="CRATE_HEAP_SIZE=15829303296"
                  Environment="CRATE_HOME=/home/ec2-user/crate"
                  Environment="CRATE_PATH_CONF=/home/ec2-user/crate"
                  Environment="CRATE_PATH_LOG=/home/ec2-user/logs"
                  Environment="CRATE_GC_LOG_DIR=/home/ec2-user/logs"
                  Environment="CRATE_HEAP_DUMP_PATH=/var/lib/crate"
                  ExecStart=/home/ec2-user/crate/bin/crate \
                      -Cpath.conf=/home/ec2-user/crate/config/ \
                      -Cpath.logs=/home/ec2-user/logs/
                  StandardOutput=journal
                  StandardError=journal
                  TimeoutStopSec=300
                  # SIGTERM signal is used to stop the Java process
                  KillSignal=SIGTERM
                  # Send the signal only to the JVM rather than its control group
                  KillMode=process
                  # Java process is never killed
                  SendSIGKILL=no
                  SuccessExitStatus=143 SIGTERM SIGKILL
                  LimitMEMLOCK=infinity
                  LimitNOFILE=65536
                  LimitNPROC=4096
                  LimitAS=infinity
                  [Install]
                  WantedBy=multi-user.target



        - on_all_hosts:
            download_files:
              # - source: https://cdn.crate.io/downloads/releases/crash_standalone_0.23.0
              - source: https://cdn.crate.io/downloads/releases/crash_standalone_latest
                target: crash
        - on_all_hosts:
            exec: |
                sudo cp crate.service /lib/systemd/system/crate.service
                chmod +x crash
                ./crash --version

      post_cluster_start:
        - on_workload_client:
            exec_client_shell:
              connection_string: ${cluster_setup.meta.connection_string}/test
              script: |
                  CREATE USER username WITH (password='password');
                  GRANT ALL PRIVILEGES to username;


      post_test:
        - on_workload_client:
            exec: |
              ./crash --format dynamic --hosts md:4200 --command "SELECT * FROM sys.jobs_metrics LIMIT 100;"> ./sys.jobs_metrics.out
              ./crash --format dynamic --hosts md:4200 --command "SELECT stmt, SUM(ended-started) as sum_t, COUNT(stmt) as num_t, MAX(ended-started) as max_t, AVG(ended-started) as avg_t, MIN(ended-started) as min_t FROM sys.jobs_log GROUP BY stmt ORDER BY sum_t DESC LIMIT 100;" > ./sys.jobs_log.aggregated.out
              # ./crash --format dynamic --hosts ${cluster_setup.meta.connection_string} --command "SELECT * FROM sys.jobs_log;" |gzip> ./sys.jobs_log.out.gz
              # ./crash --format dynamic --hosts ${cluster_setup.meta.connection_string} --command "SELECT * FROM sys.operations_log;" |gzip> ./sys.operations_log.out.gz

        - on_all_servers:
            exec: |
              # sadf also comes with sysstat. And here it's guaranteed to be a compatible version with sar.
              # Easiest to just generate graphs immediately after each test.
              cd ${cluster_setup.directories.log_dir}
              sadf -g sar_datafile.bin.log -- -B > sar_paging-B.svg
              sadf -g sar_datafile.bin.log -- -b > sar_blocks-b.svg
              sadf -g sar_datafile.bin.log -- -d > sar_iostat-d.svg
              sadf -g sar_datafile.bin.log -- -F > sar_mounts-F.svg
              sadf -g sar_datafile.bin.log -- -H > sar_hugepage-H.svg
              sadf -g sar_datafile.bin.log -- -q > sar_load-q.svg
              sadf -g sar_datafile.bin.log -- -r > sar_memory-r.svg
              sadf -g sar_datafile.bin.log -- -S > sar_swap-S.svg
              sadf -g sar_datafile.bin.log -- -u > sar_cpu_util-u.svg
              sadf -g sar_datafile.bin.log -- -v > sar_inode-v.svg
              sadf -g sar_datafile.bin.log -- -W > sar_swap-W.svg

        - on_all_servers:
            retrieve_files:
              - source: ${cluster_setup.directories.log_dir}
                target: ./
        - on_workload_client:
            retrieve_files:
              - source: ./sys.jobs_log.aggregated.out
                target: ./sys.jobs_log.aggregated.out
        - on_workload_client:
            retrieve_files:
              - source: ./sys.jobs_metrics.out
                target: ./sys.jobs_metrics.out

  post_task:
    - on_all_servers:
        exec: |
          rm -rf ./mdiag
          mkdir -p ./mdiag
          # TODO: Periodically check forn new stuff in upstream
          # wget -q -O-  https://raw.githubusercontent.com/mongodb/support-tools/master/mdiag/mdiag.sh > ./mdiag/mdiag.sh
          wget -q -O-  https://raw.githubusercontent.com/henrikingo/support-tools/master/mdiag/mdiag.sh > ./mdiag/mdiag.sh
          # The output is written to "$TMPDIR/mdiag-$(hostname).json".
          # This json file is machine readable, rendering the
          # output to a more hunan friendly format is not done here.
          echo "Collecting config and perf metrics with mdiag."
          sudo TMPDIR=./mdiag bash ./mdiag/mdiag.sh --inhibit-new-version-check --fast > /dev/null
    - on_all_servers:
        retrieve_files:
          - source: mdiag/
            target: ./

  upon_error:
    - on_all_servers:
        exec: |
          (df; du  --max-depth 1 ./data) | sed "s/^/$(hostname): /"
    - on_all_servers:
        retrieve_files:
          - source: data/logs/
            target: ./
    - on_all_servers:
        retrieve_files:
          - source: ./logs/
            target: ./

test_control:
  task_name: default_task_name
  timeouts:
    no_output_ms: 5400000  # 90 minutes
  jstests_dir: ./jstests/hooks
  between_tests:
    - restart_cluster:
        clean_logs: true
        clean_db_dir: true

  background_tasks:
    - on_all_servers:
        sar: sar -A -o ${cluster_setup.directories.log_dir}/sar_datafile.bin.log 1




  # DSI can listen on a port on the workload_client and receive commands of the same format as in
  # pre_test, post_test, etc. (In particular, restart_mongodb.)
  dsisocket:
    enabled: false
    bind_addr: 127.0.0.1  # This is local on workload_client, with reverse ssh tunnel back to dsi control host.
    port: 27007

  output_file:
    mongoshell: test_output.log
    ycsb: test_output.log
    fio: fio.json
    iperf: iperf.json
    tpcc: results.log
    genny: genny-perf.json
    sysbench: test_output.log

  # As I'm writing this, the basename IS the name. PERF-1096 will add a unique component.
  reports_dir_basename: reports
  perf_json:
    path: perf.json

  product:
    mongodb:
      ycsb:
        binding_name: mongodb
        create_table_script: |
          db.getSiblingDB("ycsb").dropDatabase();
          ${test_control.ycsb_sharding_script}
        dbproperties: |
            mongodb.url=${mongodb_setup.meta.mongodb_url}

    cratedb:
      ycsb:
        binding_name: jdbc
        create_table_script: |
            DROP TABLE IF EXISTS ycsb.usertable;
            CREATE TABLE ycsb.usertable (
                  YCSB_KEY TEXT PRIMARY KEY,
                  FIELD0 TEXT, FIELD1 TEXT,
                  FIELD2 TEXT, FIELD3 TEXT,
                  FIELD4 TEXT, FIELD5 TEXT,
                  FIELD6 TEXT, FIELD7 TEXT,
                  FIELD8 TEXT, FIELD9 TEXT
              );
        dbproperties: |
              db.driver=org.postgresql.Driver
              db.url=jdbc:postgresql://${cluster_setup.meta.hostname}:${cluster_setup.meta.pgsql.port}/ycsb
              db.user=username
              db.passwd=password

  common_fio_config: |
    [global]
    directory=./data/fio
    filename_format=fiofile.$jobnum.$filenum
    size=1G
    runtime=120
    time_based
    group_reporting
    ioengine=libaio
    direct=1

    [setupfiles]
    stonewall
    filesize=1G
    nrfiles=16
    filename_format=fiofile.$filenum.0
    rw=randrw
    numjobs=1
    runtime=1

    [latency_test]
    stonewall
    description=This is for random read and write latency. 1 at a time
    rw=randrw
    numjobs=1
    bs=1
    ioengine=sync
    direct=0
    filesize=1G
    nrfiles=16
    filename_format=fiofile.$filenum.0
    write_bw_log=fiolatency
    write_lat_log=fiolatency
    write_iops_log=fiolatency

    [iops_test]
    stonewall
    description=How many iops can I sustain in parallel
    rw=randrw
    numjobs=32
    bs=4k
    iodepth=32
    write_bw_log=fioiops
    write_lat_log=fioiops
    write_iops_log=fioiops

    [streaming_bandwidth_test]
    stonewall
    description=Measure streaming bandwidth
    rw=rw
    numjobs=16
    bs=16k
    iodepth=32
    write_bw_log=fiostreaming
    write_lat_log=fiostreaming
    write_iops_log=fiostreaming

  common_fio_net_config: |
    [global]
    ioengine=net
    port=27019
    protocol=tcp
    bs=4k
    size=1g
    runtime=20
    time_based
    pingpong=1

    [sender]
    hostname=${mongodb_setup.meta.hostname}
    rw=write

  common_fio_net_config_listener: |
    [global]
    ioengine=net
    port=27019
    protocol=tcp
    bs=4k
    size=1g
    runtime=40
    time_based
    pingpong=1

    [receiver]
    listen
    rw=read

  ycsb_sharding_script: |
    if ("${mongodb_setup.meta.is_sharded}" == "True") {
      (function () {
        var err;
        for (var i = 0; i < 20; i++) {
          try {
            assert.commandWorked(sh.enableSharding("ycsb"));
            assert.commandWorked(
              sh.shardCollection("ycsb.usertable", {_id: "hashed"}));
            db.printShardingStatus();
            return;
          } catch (e) {
            err = e;
            sleep(1000);
          }
        }
        throw err;
      })()
    } else {
      print ("Non-sharded cluster");
    }

  bestbuy_sharding_script: |
    if ("${mongodb_setup.meta.is_sharded}" == "True") {

      // The following variables that are specific to the workload.
      // NOTE: the splits must be in order.
      var dbName = 'bestbuy';
      var sourceNs = 'bestbuy.products';

      // In addition to sharding the main namespace of the workload, we also set up some collections
      // which are initially empty but meant to be targeted by an $out stage during the test.
      var targetNsIdenticalDistributionSameDB = 'bestbuy.target_identical_distribution';
      var targetNsRangeIdSameDB = 'bestbuy.target_range_id';
      var targetDBName = 'target';
      var targetNsIdenticalDistributionOtherDB = 'target.identical_distribution';
      var targetNsIdHashedOtherDB = 'target.hashed_id';

      var keyPattern = {type: 1, productId: 1, _id: 1};
      var typeAndProductSplits = [
        {type: MinKey,     productId: MinKey, _id: 0},
        {type: 'Game',     productId: NumberLong('1218086489034'), _id: 0},
        {type: 'HardGood', productId: NumberLong('1118840368488'), _id: 0},
        {type: 'HardGood', productId: NumberLong('1218105505555'), _id: 0},
        {type: 'HardGood', productId: NumberLong('1218379812886'), _id: 0},
        {type: 'HardGood', productId: NumberLong('1218475610897'), _id: 0},
        {type: 'HardGood', productId: NumberLong('1218618813745'), _id: 0},
        {type: 'HardGood', productId: NumberLong('1218728444211'), _id: 0}, // 8,9
        {type: 'HardGood', productId: NumberLong('1219080834391'), _id: 0},
        {type: 'HardGood', productId: NumberLong('1219736622160'), _id: 0}, // 7
        {type: 'Movie',    productId: NumberLong('28132'), _id: 0},         // 6
        {type: 'Movie',    productId: NumberLong('44465'), _id: 0},
        {type: 'Movie',    productId: NumberLong('59159'), _id: 0},         // 5
        {type: 'Movie',    productId: NumberLong('1344253'), _id: 0},
        {type: 'Movie',    productId: NumberLong('1459047'), _id: 0},       // 8,9
        {type: 'Movie',    productId: NumberLong('1627489'), _id: 0},       // 4
        {type: 'Movie',    productId: NumberLong('1921771'), _id: 0},
        {type: 'Movie',    productId: NumberLong('2180510'), _id: 0},
        {type: 'Movie',    productId: NumberLong('2702290'), _id: 0},       // 7
        {type: 'Movie',    productId: NumberLong('3313579'), _id: 0},
        {type: 'Music',    productId: NumberLong('73927'), _id: 0},         // 3,6
        {type: 'Music',    productId: NumberLong('97408'), _id: 0},         // 8,9
        {type: 'Music',    productId: NumberLong('120846'), _id: 0},
        {type: 'Music',    productId: NumberLong('145563'), _id: 0},
        {type: 'Music',    productId: NumberLong('172542'), _id: 0},        // 5
        {type: 'Music',    productId: NumberLong('199025'), _id: 0},
        {type: 'Music',    productId: NumberLong('226846'), _id: 0},
        {type: 'Music',    productId: NumberLong('261778'), _id: 0},        // 7
        {type: 'Music',    productId: NumberLong('301467'), _id: 0},        // 8,9
        {type: 'Music',    productId: NumberLong('1337396'), _id: 0},
        {type: 'Music',    productId: NumberLong('1393391'), _id: 0},       // 4,6
        {type: 'Music',    productId: NumberLong('1443146'), _id: 0},
        {type: 'Music',    productId: NumberLong('1484664'), _id: 0},
        {type: 'Music',    productId: NumberLong('1531482'), _id: 0},
        {type: 'Music',    productId: NumberLong('1582679'), _id: 0},
        {type: 'Music',    productId: NumberLong('1624125'), _id: 0},       // 8,9
        {type: 'Music',    productId: NumberLong('1686112'), _id: 0},       // 5,7
        {type: 'Music',    productId: NumberLong('1778529'), _id: 0},
        {type: 'Music',    productId: NumberLong('1815087'), _id: 0},
        {type: 'Music',    productId: NumberLong('1858555'), _id: 0},
        {type: 'Music',    productId: NumberLong('1920647'), _id: 0},       // 3,6
        {type: 'Music',    productId: NumberLong('1992906'), _id: 0},
        {type: 'Music',    productId: NumberLong('2067260'), _id: 0},       // 8,9
        {type: 'Music',    productId: NumberLong('2149519'), _id: 0},
        {type: 'Music',    productId: NumberLong('2229905'), _id: 0},
        {type: 'Music',    productId: NumberLong('2305925'), _id: 0},       // 4
        {type: 'Music',    productId: NumberLong('2428992'), _id: 0},
        {type: 'Music',    productId: NumberLong('2558418'), _id: 0},
        {type: 'Music',    productId: NumberLong('2656772'), _id: 0},       // 5,7
        {type: 'Music',    productId: NumberLong('2709433'), _id: 0},       // 8,9
        {type: 'Music',    productId: NumberLong('2776923'), _id: 0},       // 6
        {type: 'Music',    productId: NumberLong('2820755'), _id: 0},
        {type: 'Music',    productId: NumberLong('2864319'), _id: 0},
        {type: 'Music',    productId: NumberLong('2900297'), _id: 0},
        {type: 'Music',    productId: NumberLong('2933428'), _id: 0},
        {type: 'Music',    productId: NumberLong('2971126'), _id: 0},
        {type: 'Music',    productId: NumberLong('3230451'), _id: 0},       // 7,8,9
        {type: 'Music',    productId: NumberLong('3310959'), _id: 0},
        {type: 'Music',    productId: NumberLong('3383162'), _id: 0},
        {type: 'Music',    productId: NumberLong('3514683'), _id: 0},
      ];

      // Get a list of the shard names.
      var config = db.getSiblingDB("config");
      var shards = config.shards.find({},{_id:1}).map(function(s){return s._id;});

      // Utility function to split a chunk in the middle.
      // See https://docs.mongodb.com/manual/reference/command/split/.
      // The split document must contain:
      //     ns {string} the collection names.
      //     middle {document} the split point.
      var splitter = function(split){
        return db.adminCommand({split: split.ns, middle: split.middle});
      };
      // Utility function to move a chunk.
      // See https://docs.mongodb.com/manual/reference/command/moveChunk/
      // The split document must contain:
      //     ns {string} the collection names
      //     middle {document} the split point
      //     shard {string} the destination shard
      var mover = function(split){
        return db.adminCommand({moveChunk: split.ns, find: split.middle, to: split.shard});
      };

      assert.commandWorked(sh.enableSharding(dbName));
      assert.commandWorked(sh.enableSharding(targetDBName));
      assert.commandWorked(sh.shardCollection(sourceNs, keyPattern));
      assert.commandWorked(sh.shardCollection(targetNsIdenticalDistributionSameDB, keyPattern, /*unique=*/true));
      assert.commandWorked(sh.shardCollection(targetNsIdenticalDistributionOtherDB, keyPattern, /*unique=*/true));

      // For a shard key {_id: "hashed"}, MongoDB will automatically split the collection into
      // two chunks per shard so no further work is necessary.
      assert.commandWorked(sh.shardCollection(targetNsIdHashedOtherDB, {_id: "hashed"}));

      var idAlphabetSplits = "abcdefghijklmnopqrstuvwxyz".split("").map(function(letter) {
          return {_id: letter};
      });
      assert.commandWorked(sh.shardCollection(targetNsRangeIdSameDB, {_id: 1}));

      var distributeColl = function (ns, splits) {

        // Given the split points 'splits' and 'nShards' shards, we try to partition the data such
        // that each shard has one chunk, but each chunk is of a similar size. Assuming there is a
        // similar amount of data between each split point, this means dividing the splits into
        // 'nShards' equal chunks.
        //
        // For example, if there are 31 entries in 'splits' and 3 shards, we take the 0th, 11th,
        // and 22nd entries, resulting in a filtered array of length 3.
        //
        // The map function creates a new document of the format required by the splitter and mover
        // functions. That is, it creates a document with:
        //    shard {string} the destination shard
        //    ns {string} the fully qualified collection namespace
        //    middle {document} the split point
        //
        // For a reasonable number of splits filter is ok. For a very large number
        // it will perform poorly. BUT, even in this case, it will probably be negligible.
        var distribution =  splits.filter(function(element, index){
          return index % Math.ceil(splits.length / shards.length) == 0;
        }).map(function(middle, i) {
          return {shard:shards[i % shards.length], middle:middle, ns: ns};
        });

        // Create the split points.
        // NOTE: to create N chunks, you only need N - 1 split points.
        distribution.slice(1).forEach(function(split) {
          assert.commandWorked(splitter(split));
        });

        // Move the chunks to specific shards.
        distribution.forEach(function(split){ assert.commandWorked(mover(split));});
      };

      // Disable balancing. If the split points are fair, then the cluster will still be in balance
      // at the end of the restore.
      sh.stopBalancer();

      distributeColl(sourceNs, typeAndProductSplits);
      distributeColl(targetNsIdenticalDistributionSameDB, typeAndProductSplits);
      distributeColl(targetNsIdenticalDistributionOtherDB, typeAndProductSplits);
      distributeColl(targetNsRangeIdSameDB, idAlphabetSplits);

      db.printShardingStatus();
    } else {
      print ("Non-sharded cluster");
    }

# add a delay before each test. Units are seconds but the value can be an int or float.
#  test_delay_seconds: 1

  start_mongo_cryptd: |
    # Start mongocryptd separately so that it runs on a different cpu
    pwd
    if [ "${mongodb_setup.meta.is_fle}" == "True" ]; then
      ${mongodb_setup.mongo_dir}/bin/mongocryptd --logpath $HOME/cryptd.log --pidfilepath=$HOME/mongocryptd.pid --fork
    fi

  # Common genny commands
  genny:
    mkdir: |
      set -eou pipefail
      # Write the output of genny to the ephemeral drive to ensure it has enough disk space.
      cd ./data/genny
      mkdir -p metrics

    # Just append workload yml file when using this
    exec: ${infrastructure_provisioning.numactl_prefix} ./scripts/genny run -u "${mongodb_setup.meta.mongodb_url}" -m cedar-csv -o ./genny-perf.csv

    metrics: |
      genny-metrics-legacy-report --report-file genny-perf.json genny-perf.csv

  cwrwc_setup_script: |
    (function () {
      var err;
      for (var i = 0; i < 20; i++) {
        try {
          // Set the CWRWC defaults to be the same as the implicit server defaults, which should
          // perform equivalently to when the defaults are never set.
          printjson(assert.commandWorked(db.adminCommand({
              setDefaultRWConcern: 1,
              defaultReadConcern: {level: "local"},
              defaultWriteConcern: {w: 1, wtimeout: 0}})));
          return;
        } catch (e) {
          err = e;
          sleep(1000);
        }
      }
      throw err;
    })();
